{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Challenge: Implement PatternCount() (reproduced below).\n",
    "#      Input: Strings Text and Pattern.\n",
    "#      Output: Count(Text, Pattern).\n",
    "\n",
    "import sys\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your PatternCount function here, along with any subroutines you need\n",
    "def pattern_count(text: str, pattern: str) -> int:\n",
    "    count = 0\n",
    "    for i in range(len(text)):\n",
    "        if (text[i:len(pattern) + i]) == pattern:\n",
    "            count+=1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Challenge: Solve the Frequent Words Problem.\n",
    "\n",
    "# Input: A string Text and an integer k.\n",
    "# Output: All most frequent k-mers in Text.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your frequent_words function here, along with any subroutines you need\n",
    "def frequent_words(text: str, k: int) -> list[str]:\n",
    "    \"\"\"Find the most frequent k-mers in a given text.\"\"\"\n",
    "    freqMap = {}\n",
    "    n = len(text)\n",
    "    for i in range(n):\n",
    "        pattern = text[i:k+i]\n",
    "        if pattern in freqMap.keys():\n",
    "            freqMap[pattern] += 1\n",
    "        else:\n",
    "            freqMap[pattern] = 1\n",
    "    max_val = max(freqMap.values())\n",
    "    frequent_patterns = list(filter(lambda x: freqMap[x] == max_val, freqMap))\n",
    "    return frequent_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse Complement Problem: Find the reverse complement of a DNA string.\n",
    "\n",
    "# Input: A DNA string Pattern.\n",
    "# Output: Patternrc , the reverse complement of Pattern.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your reverse_complement function here, along with any subroutines you need\n",
    "def reverse_complement(pattern: str) -> str:\n",
    "    \"\"\"Calculate the reverse complement of a DNA pattern.\"\"\"\n",
    "    rev_str = pattern[::-1]\n",
    "    c_map = {\"A\": \"T\", \"C\": \"G\", \"T\": \"A\", \"G\": \"C\"}\n",
    "    rev_comp = \"\"\n",
    "    return rev_comp.join(c_map[base] for base in rev_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Challenge: Solve the Pattern Matching Problem.\n",
    "\n",
    "# Input: Two strings, Pattern and Genome.\n",
    "# Output: A collection of integers specifying all starting positions where Pattern appears as a substring of Genome.\n",
    "\n",
    "import sys\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your pattern_matching function here, along with any subroutines you need\n",
    "def pattern_matching(pattern: str, genome: str) -> list[int]:\n",
    "    \"\"\"Find all occurrences of a pattern in a genome.\"\"\"\n",
    "    p_locs = []\n",
    "    for i in range(len(genome)):\n",
    "        if (genome[i:len(pattern) + i]) == pattern:\n",
    "            p_locs.append(i)\n",
    "    return p_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the text file Vibrio_cholerae.txt\n",
    "# find the number of times the pattern \"CTTGATCAT\" occurs in the text file\n",
    "# return a space separated list of the starting positions of the pattern in the text file\n",
    "\n",
    "import sys\n",
    "import re # regular expressions\n",
    "\n",
    "def get_positions():\n",
    "    file_loc = \"data/Vibrio_cholerae.txt\"\n",
    "    with open(file_loc, 'r') as f:\n",
    "        text = f.read()\n",
    "    pattern = \"CTTGATCAT\"\n",
    "    positions = [str(m.start()) for m in re.finditer(pattern, text)] # regex here is more efficient than using a loop\n",
    "    # explanation: re.finditer returns an iterator of match objects\n",
    "    # m.start() returns the starting index of the match\n",
    "    # we convert the starting index to a string and store it in a list\n",
    "    # save the positions as a space separated text file\n",
    "    with open('output.txt', 'w') as f:\n",
    "        f.write(\" \".join(positions))\n",
    "    print(\" \".join(positions))\n",
    "\n",
    "get_positions()\n",
    "\n",
    "# now same as above but not using regex\n",
    "\n",
    "def solve2():\n",
    "    file_loc = \"data/Vibrio_cholerae.txt\"\n",
    "    with open(file_loc, 'r') as f:\n",
    "        text = f.read()\n",
    "    pattern = \"CTTGATCAT\"\n",
    "    positions = []\n",
    "    for i in range(len(text) - len(pattern) + 1):\n",
    "        if text[i:i+len(pattern)] == pattern:\n",
    "            positions.append(str(i))\n",
    "    with open('output2.txt', 'w') as f:\n",
    "        f.write(\" \".join(positions))\n",
    "    print(\" \".join(positions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FindClumps(Text, k, L, t)\n",
    "#     Patterns ← an array of strings of length 0\n",
    "#     n ← |Text|\n",
    "#     for every integer i between 0 and n − L\n",
    "#         Window ← Text(i, L)\n",
    "#         freqMap ← FrequencyTable(Window, k)\n",
    "#         for every key s in freqMap\n",
    "#             if freqMap[s] ≥ t\n",
    "#                 append s to Patterns\n",
    "#     remove duplicates from Patterns\n",
    "#     return Patterns\n",
    "\n",
    "\n",
    "# Code Challenge: Solve the Clump Finding Problem (restated below).\n",
    "\n",
    "# Clump Finding Problem: Find patterns forming clumps in a string.\n",
    "\n",
    "# Input: A string Genome, and integers k, L, and t.\n",
    "# Output: All distinct k-mers forming (L, t)-clumps in Genome.\n",
    "\n",
    "# Sample Input:\n",
    "\n",
    "# CGGACTCGACAGATGTGAAGAACGACAATGTGAAGACTCGACACGACAGAGTGAAGAGAAGAGGAAACATTGTAA\n",
    "# 5 50 4\n",
    "# Sample Output:\n",
    "\n",
    "# GAAGA CGACA\n",
    "\n",
    "import sys\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your clump_finding function here, along with any subroutines you need\n",
    "\n",
    "def frequency_table(text: str, k: int) -> dict[str, int]:\n",
    "    \"\"\"Create a frequency table of k-mers in a given text.\"\"\"\n",
    "    freqMap = {}\n",
    "    n = len(text)\n",
    "    for i in range(n - k + 1):\n",
    "        pattern = text[i:i + k]\n",
    "        freqMap[pattern] = freqMap.get(pattern, 0) + 1\n",
    "    return freqMap\n",
    "\n",
    "def clump_finding(genome: str, k: int, L: int, t: int) -> list[str]:\n",
    "    \"\"\"Find patterns forming clumps in a string.\"\"\"\n",
    "    patterns = set()\n",
    "    n = len(genome)\n",
    "    \n",
    "    # Create frequency table for the first window\n",
    "    window = genome[:L]\n",
    "    freqMap = frequency_table(window, k)\n",
    "    \n",
    "    # Check patterns in the first window\n",
    "    for pattern, count in freqMap.items():\n",
    "        if count >= t:\n",
    "            patterns.add(pattern)\n",
    "    \n",
    "    # Slide the window and update frequency table\n",
    "    for i in range(1, n - L + 1):\n",
    "        # Remove the first k-mer of the previous window\n",
    "        first_kmer = genome[i-1:i-1+k]\n",
    "        freqMap[first_kmer] -= 1\n",
    "        if freqMap[first_kmer] == 0:\n",
    "            del freqMap[first_kmer]\n",
    "        \n",
    "        # Add the last k-mer of the new window\n",
    "        last_kmer = genome[i+L-k:i+L]\n",
    "        freqMap[last_kmer] = freqMap.get(last_kmer, 0) + 1\n",
    "        \n",
    "        # Check if the new k-mer forms a clump\n",
    "        if freqMap[last_kmer] >= t:\n",
    "            patterns.add(last_kmer)\n",
    "    \n",
    "    return list(patterns)\n",
    "\n",
    "# test with the E_coli.txt genome\n",
    "def test_clump_finding():\n",
    "    file_loc = \"data/E_coli.txt\"\n",
    "    with open(file_loc, 'r') as f:\n",
    "        genome = f.read()\n",
    "    k, L, t = 9, 500, 3\n",
    "    patterns = clump_finding(genome, k, L, t)\n",
    "    print(\" \".join(patterns))\n",
    "\n",
    "# test_clump_finding()\n",
    "# Exercise Break: How many different 9-mers form (500,3)-clumps in the E. coli genome? (In other words, do not count a 9-mer more than once.)\n",
    "\n",
    "genome = open(\"data/E_coli.txt\").read()\n",
    "num_diff_9mers = len(clump_finding(genome, 9, 500, 3))\n",
    "print(num_diff_9mers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum Skew Problem: Find a position in a genome where the skew diagram attains a minimum.\n",
    "\n",
    "# Input: A DNA string Genome.\n",
    "# Output: All integer(s) i minimizing Skewi (Genome) among all values of i (from 0 to |Genome|).\n",
    "# Code Challenge: Solve the Minimum Skew Problem.\n",
    "\n",
    "# Sample Input:\n",
    "\n",
    "# TAAAGACTGCCGAGAGGCCAACACGAGTGCTAGAACGAGGGGCGTAAACGCGGGTCCGAT\n",
    "# Sample Output:\n",
    "\n",
    "# 11 24\n",
    "\n",
    "import sys\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your minimum_skew function here, along with any subroutines you need\n",
    "\n",
    "def minimum_skew(genome: str) -> list[int]:\n",
    "    \"\"\"Find a position in a genome where the skew diagram attains a minimum.\"\"\"\n",
    "    skew = [0]\n",
    "    for i in range(len(genome)):\n",
    "        if genome[i] == \"C\":\n",
    "            skew.append(skew[i] - 1)\n",
    "        elif genome[i] == \"G\":\n",
    "            skew.append(skew[i] + 1)\n",
    "        else:\n",
    "            skew.append(skew[i])\n",
    "    min_skew = min(skew)\n",
    "    return [i for i, s in enumerate(skew) if s == min_skew]\n",
    "\n",
    "# test with the sample input\n",
    "def test_minimum_skew():\n",
    "    genome = \"TAAAGACTGCCGAGAGGCCAACACGAGTGCTAGAACGAGGGGCGTAAACGCGGGTCCGAT\"\n",
    "    positions = minimum_skew(genome)\n",
    "    print(\" \".join(map(str, positions)))\n",
    "\n",
    "test_minimum_skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We say that position i in k-mers p1 … pk and q1 … qk is a mismatch if pi ≠ qi. For example, CGAAT and CGGAC have two mismatches. The number of mismatches between strings p and q is called the Hamming distance between these strings and is denoted HammingDistance(p, q).\n",
    "\n",
    "# Hamming Distance Problem: Compute the Hamming distance between two strings.\n",
    "\n",
    "# Input: Two strings of equal length.\n",
    "# Output: The Hamming distance between these strings.\n",
    "# Code Challenge: Solve the Hamming Distance Problem.\n",
    "\n",
    "# Sample Input 1:\n",
    "\n",
    "# GGGCCGTTGGT\n",
    "# GGACCGTTGAC\n",
    "# Sample Output 1:\n",
    "\n",
    "# 3\n",
    "# Sample Input 2:\n",
    "\n",
    "# AAAA\n",
    "# TTTT\n",
    "# Sample Output 2:\n",
    "\n",
    "# 4\n",
    "# Sample Input 3:\n",
    "\n",
    "# ACGTACGT\n",
    "# TACGTACG\n",
    "# Sample Output 3:\n",
    "\n",
    "# 8\n",
    "# Sample Input 4:\n",
    "\n",
    "# ACGTACGT\n",
    "# CCCCCCCC\n",
    "# Sample Output 4:\n",
    "\n",
    "# 6\n",
    "# Sample Input 5:\n",
    "\n",
    "# ACGTACGT\n",
    "# TGCATGCA\n",
    "# Sample Output 5:\n",
    "\n",
    "# 8\n",
    "# Sample Input 6:\n",
    "\n",
    "# GATAGCAGCTTCTGAACTGGTTACCTGCCGTGAGTAAATTAAAATTTTATTGACTTAGGTCACTAAATACT\n",
    "# AATAGCAGCTTCTCAACTGGTTACCTCGTATGAGTAAATTAGGTCATTATTGACTCAGGTCACTAACGTCT\n",
    "# Sample Output 6:\n",
    "\n",
    "# 15\n",
    "# Sample Input 7:\n",
    "\n",
    "# AGAAACAGACCGCTATGTTCAACGATTTGTTTTATCTCGTCACCGGGATATTGCGGCCACTCATCGGTCAGTTGATTACGCAGGGCGTAAATCGCCAGAATCAGGCTG\n",
    "# AGAAACCCACCGCTAAAAACAACGATTTGCGTAGTCAGGTCACCGGGATATTGCGGCCACTAAGGCCTTGGATGATTACGCAGAACGTATTGACCCAGAATCAGGCTC\n",
    "# Sample Output 7:\n",
    "\n",
    "# 28\n",
    "\n",
    "import sys\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your hamming_distance function here, along with any subroutines you need\n",
    "\n",
    "def hamming_distance(p: str, q: str) -> int:\n",
    "    \"\"\"Compute the Hamming distance between two strings.\"\"\"\n",
    "    return sum(1 for pi, qi in zip(p, q) if pi != qi)\n",
    "\n",
    "# test with the sample input\n",
    "def test_hamming_distance():\n",
    "    p = \"GGGCCGTTGGT\"\n",
    "    q = \"GGACCGTTGAC\"\n",
    "    print(hamming_distance(p, q))\n",
    "\n",
    "test_hamming_distance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We say that a k-mer Pattern appears as a substring of Text with at most d mismatches if there is some k-mer substring Pattern' of Text having d or fewer mismatches with Pattern, i.e., HammingDistance(Pattern, Pattern') ≤ d. Our observation that a DnaA box may appear with slight variations leads to the following generalization of the Pattern Matching Problem.\n",
    "\n",
    "# Approximate Pattern Matching Problem: Find all approximate occurrences of a pattern in a string.\n",
    "\n",
    "# Input: Strings Pattern and Text along with an integer d.\n",
    "# Output: All starting positions where Pattern appears as a substring of Text with at most d mismatches.\n",
    "# Code Challenge: Solve the Approximate Pattern Matching Problem.\n",
    "\n",
    "# Sample Input:\n",
    "\n",
    "# ATTCTGGA\n",
    "# CGCCCGAATCCAGAACGCATTCCCATATTTCGGGACCACTGGCCTCCACGGTACGGACGTCAATCAAAT\n",
    "# 3\n",
    "# Sample Output:\n",
    "\n",
    "# 6 7 26 27\n",
    "\n",
    "import sys\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your approximate_pattern_matching function here, along with any subroutines you need\n",
    "\n",
    "def approximate_pattern_matching(pattern: str, text: str, d: int) -> list[int]:\n",
    "    \"\"\"Find all approximate occurrences of a pattern in a string.\"\"\"\n",
    "    positions = []\n",
    "    k = len(pattern)\n",
    "    for i in range(len(text) - k + 1):\n",
    "        if hamming_distance(pattern, text[i:i+k]) <= d:\n",
    "            positions.append(i)\n",
    "    return positions\n",
    "\n",
    "# test with the sample input\n",
    "def test_approximate_pattern_matching():\n",
    "    pattern = \"ATTCTGGA\"\n",
    "    text = \"CGCCCGAATCCAGAACGCATTCCCATATTTCGGGACCACTGGCCTCCACGGTACGGACGTCAATCAAAT\"\n",
    "    d = 3\n",
    "    positions = approximate_pattern_matching(pattern, text, d)\n",
    "    print(\" \".join(map(str, positions)))\n",
    "\n",
    "test_approximate_pattern_matching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ApproximatePatternCount(Text, Pattern, d)\n",
    "#     count ← 0\n",
    "#     for i ← 0 to |Text| − |Pattern|\n",
    "#         Pattern′ ← Text(i , |Pattern|)\n",
    "#         if HammingDistance(Pattern, Pattern′) ≤ d\n",
    "#             count ← count + 1\n",
    "#     return count\n",
    "# Code Challenge: Implement ApproximatePatternCount().\n",
    "\n",
    "# Input: Strings Pattern and Text as well as an integer d.\n",
    "# Output: Countd(Text, Pattern).\n",
    "\n",
    "# Sample Input:\n",
    "\n",
    "# GAGG\n",
    "# TTTAGAGCCTTCAGAGG\n",
    "# 2\n",
    "# Sample Output:\n",
    "\n",
    "# 4\n",
    "\n",
    "import sys\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your approximate_pattern_count function here, along with any subroutines you need\n",
    "\n",
    "def approximate_pattern_count(text: str, pattern: str, d: int) -> int:\n",
    "    \"\"\"Count the number of approximate occurrences of a pattern in a text.\"\"\"\n",
    "    count = 0\n",
    "    k = len(pattern)\n",
    "    for i in range(len(text) - k + 1):\n",
    "        if hamming_distance(pattern, text[i:i+k]) <= d:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# test with the sample input\n",
    "def test_approximate_pattern_count():\n",
    "    text = \"TTTAGAGCCTTCAGAGG\"\n",
    "    pattern = \"GAGG\"\n",
    "    d = 2\n",
    "    print(approximate_pattern_count(text, pattern, d))\n",
    "\n",
    "test_approximate_pattern_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code Challenge: Implement MotifEnumeration() (reproduced below).\n",
    "\n",
    "# Input: Integers k and d, followed by a space-separated collection of strings Dna.\n",
    "# Output: All (k, d)-motifs in Dna.\n",
    "# MotifEnumeration(Dna, k, d)\n",
    "#     Patterns ← an empty set\n",
    "#     for each k-mer Pattern in first string in Dna\n",
    "#         for each k-mer Pattern’ differing from Pattern by at most d mismatches\n",
    "#             if Pattern' appears in each string from Dna with at most d mismatches\n",
    "#                 add Pattern' to Patterns\n",
    "#     remove duplicates from Patterns\n",
    "#     return Patterns\n",
    "\n",
    "# Sample Input:\n",
    "\n",
    "# 3 1\n",
    "# ATTTGGC TGCCTTA CGGTATC GAAAATT\n",
    "# Sample Output:\n",
    "\n",
    "# GTT TTT ATA ATT\n",
    "\n",
    "import sys\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your motif_enumeration function here, along with any subroutines you need\n",
    "\n",
    "def hamming_distance(p: str, q: str) -> int:\n",
    "    \"\"\"Compute the Hamming distance between two strings.\"\"\"\n",
    "    return sum(1 for pi, qi in zip(p, q) if pi != qi)\n",
    "\n",
    "def neighbors(pattern: str, d: int) -> set[str]:\n",
    "    \"\"\"Generate all k-mers within Hamming distance d of a pattern.\"\"\"\n",
    "    if d == 0:\n",
    "        return {pattern}\n",
    "    if len(pattern) == 1:\n",
    "        return {\"A\", \"C\", \"G\", \"T\"}\n",
    "    neighborhood = set()\n",
    "    suffix_neighbors = neighbors(pattern[1:], d)\n",
    "    for text in suffix_neighbors:\n",
    "        if hamming_distance(pattern[1:], text) < d:\n",
    "            for base in \"ACGT\":\n",
    "                neighborhood.add(base + text)\n",
    "        else:\n",
    "            neighborhood.add(pattern[0] + text)\n",
    "    return neighborhood\n",
    "\n",
    "def motif_enumeration(dna: list[str], k: int, d: int) -> list[str]:\n",
    "    \"\"\"Find all (k, d)-motifs in a list of DNA strings.\"\"\"\n",
    "    patterns = set()\n",
    "    for i in range(len(dna[0]) - k + 1):\n",
    "        pattern = dna[0][i:i+k]\n",
    "        for neighbor in neighbors(pattern, d):\n",
    "            if all(any(hamming_distance(neighbor, dna_str[j:j+k]) <= d \n",
    "                       for j in range(len(dna_str) - k + 1)) \n",
    "                   for dna_str in dna):\n",
    "                patterns.add(neighbor)\n",
    "    return sorted(list(patterns))\n",
    "\n",
    "# Test with the sample input\n",
    "def test_motif_enumeration():\n",
    "    k, d = 3, 1\n",
    "    dna = [\"ATTTGGC\", \"TGCCTTA\", \"CGGTATC\", \"GAAAATT\"]\n",
    "    motifs = motif_enumeration(dna, k, d)\n",
    "    print(\" \".join(motifs))\n",
    "\n",
    "test_motif_enumeration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional, Ungraded) Code Challenge: Implement MedianString().\n",
    "\n",
    "# Input: An integer k, followed by a space-separated collection of strings Dna.\n",
    "# Output: A k-mer Pattern that minimizes d(Pattern, Dna) among all possible choices of k-mers. (If there are multiple such strings Pattern, then you may return any one.)\n",
    "\n",
    "# Sample Input:\n",
    "\n",
    "# 3import sys\n",
    "from typing import List, Dict, Iterable, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_paired_reads_from_file(filename: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Reads paired reads from a file and returns them as a list of tuples.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return [tuple(line.strip().split('|')) for line in f]\n",
    "\n",
    "def StringReconstructionReadPairs(PairedReads: List[Tuple[str, str]],\n",
    "                                  k: int, d: int) -> str:\n",
    "    \"\"\"\n",
    "    Reconstructs a string from its (k,d)-mer composition.\n",
    "    \n",
    "    Args:\n",
    "    PairedReads: List of tuples, each containing a pair of k-mers\n",
    "    k: Length of each k-mer in the paired reads\n",
    "    d: Gap between paired k-mers\n",
    "    \n",
    "    Returns:\n",
    "    Reconstructed string\n",
    "    \"\"\"\n",
    "    # Construct the paired de Bruijn graph\n",
    "    graph = construct_paired_de_bruijn_graph(PairedReads, k)\n",
    "    \n",
    "    # Find Eulerian path in the graph\n",
    "    path = find_eulerian_path(graph)\n",
    "    \n",
    "    # Reconstruct the string from the path\n",
    "    return reconstruct_from_paired_path(path, k, d)\n",
    "\n",
    "def construct_paired_de_bruijn_graph(PairedReads: List[Tuple[str, str]], k: int) -> Dict[Tuple[str, str], List[Tuple[str, str]]]:\n",
    "    \"\"\"Constructs a paired de Bruijn graph from the read-pairs.\"\"\"\n",
    "    graph = defaultdict(list)\n",
    "    for read1, read2 in PairedReads:\n",
    "        for i in range(len(read1) - k + 1):\n",
    "            prefix1, suffix1 = read1[i:i+k-1], read1[i+1:i+k]\n",
    "            prefix2, suffix2 = read2[i:i+k-1], read2[i+1:i+k]\n",
    "            graph[(prefix1, prefix2)].append((suffix1, suffix2))\n",
    "    return dict(graph)\n",
    "\n",
    "def find_eulerian_path(graph: Dict[Tuple[str, str], List[Tuple[str, str]]]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Finds an Eulerian path in the paired de Bruijn graph.\"\"\"\n",
    "    in_degree = defaultdict(int)\n",
    "    for node, edges in graph.items():\n",
    "        for edge in edges:\n",
    "            in_degree[edge] += 1\n",
    "    \n",
    "    start = next((node for node in graph if len(graph[node]) > in_degree[node]), None)\n",
    "    end = next((node for node in in_degree if in_degree[node] > len(graph.get(node, []))), None)\n",
    "\n",
    "    if start is None or end is None:\n",
    "        raise ValueError(\"Unable to find start or end node for Eulerian path\")\n",
    "\n",
    "    if end not in graph:\n",
    "        graph[end] = []\n",
    "    graph[end].append(start)\n",
    "\n",
    "    cycle = find_eulerian_cycle(graph)\n",
    "\n",
    "    # Rotate cycle to start with the correct edge\n",
    "    start_index = next((i for i, (u, v) in enumerate(zip(cycle, cycle[1:])) if u == end and v == start), None)\n",
    "    if start_index is None:\n",
    "        raise ValueError(\"Unable to find the correct starting point in the Eulerian cycle\")\n",
    "    \n",
    "    return cycle[start_index + 1:] + cycle[1:start_index + 1]\n",
    "\n",
    "def find_eulerian_cycle(graph: Dict[Tuple[str, str], List[Tuple[str, str]]]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Finds an Eulerian cycle in the given graph.\"\"\"\n",
    "    stack = []\n",
    "    cycle = []\n",
    "    current = next(iter(graph))\n",
    "    while stack or graph[current]:\n",
    "        if not graph[current]:\n",
    "            cycle.append(current)\n",
    "            current = stack.pop()\n",
    "        else:\n",
    "            stack.append(current)\n",
    "            next_node = graph[current].pop()\n",
    "            current = next_node\n",
    "    return cycle[::-1]\n",
    "\n",
    "def reconstruct_from_paired_path(path: List[Tuple[str, str]], k: int, d: int) -> str:\n",
    "    \"\"\"Reconstructs the string from the Eulerian path of paired k-mers.\"\"\"\n",
    "    left_path = [pair[0] for pair in path]\n",
    "    right_path = [pair[1] for pair in path]\n",
    "    \n",
    "    genome = left_path[0]\n",
    "    for node in left_path[1:]:\n",
    "        genome += node[-1]\n",
    "    \n",
    "    for i in range(k + d):\n",
    "        genome += right_path[i][-1]\n",
    "    \n",
    "    return genome\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"data/paired_end_reads.txt\"\n",
    "    PairedReads = read_paired_reads_from_file(input_file)\n",
    "    k = 25  # Adjusted k-mer length for de Bruijn graph construction\n",
    "    d = 1000  # gap between paired k-mers\n",
    "    \n",
    "    try:\n",
    "        result = StringReconstructionReadPairs(PairedReads, k, d)\n",
    "        print(f\"Length of reconstructed genome: {len(result)}\")\n",
    "        print(f\"First 100 nucleotides: {result[:100]}\")\n",
    "        print(f\"Last 100 nucleotides: {result[-100:]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "# AAATTGACGCAT GACGACCACGTT CGTCAGCGCCTG GCTGAGCACCGG AGTACGGGACAG\n",
    "# Sample Output:\n",
    "\n",
    "# ACG\n",
    "\n",
    "import sys\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your median_string function here, along with any subroutines you need\n",
    "\n",
    "def hamming_distance(p: str, q: str) -> int:\n",
    "    \"\"\"Compute the Hamming distance between two strings.\"\"\"\n",
    "    return sum(1 for pi, qi in zip(p, q) if pi != qi)\n",
    "\n",
    "def distance(pattern: str, dna: list[str]) -> int:\n",
    "    \"\"\"Compute the total Hamming distance between a pattern and a list of DNA strings.\"\"\"\n",
    "    return sum(min(hamming_distance(pattern, dna_str[i:i+len(pattern)]) for i in range(len(dna_str) - len(pattern) + 1) ) for dna_str in dna)\n",
    "\n",
    "def number_to_pattern(index: int, k: int) -> str:\n",
    "    \"\"\"Convert an index to a DNA pattern of length k.\"\"\"\n",
    "    if k == 1:\n",
    "        return \"ACGT\"[index]\n",
    "    prefix_index = index // 4\n",
    "    r = index % 4\n",
    "    symbol = \"ACGT\"[r]\n",
    "    prefix_pattern = number_to_pattern(prefix_index, k - 1)\n",
    "    return prefix_pattern + symbol\n",
    "\n",
    "def median_string(dna: list[str], k: int) -> str:\n",
    "    \"\"\"Find a k-mer Pattern that minimizes d(Pattern, Dna) among all possible choices of k-mers.\"\"\"\n",
    "    distance_min = float(\"inf\")\n",
    "    for i in range(4**k):\n",
    "        pattern = number_to_pattern(i, k)\n",
    "        if distance(pattern, dna) < distance_min:\n",
    "            distance_min = distance(pattern, dna)\n",
    "            median = pattern\n",
    "    return median\n",
    "\n",
    "# Test with the sample input\n",
    "def test_median_string():\n",
    "    k = 3\n",
    "    dna = [\"AAATTGACGCAT\", \"GACGACCACGTT\", \"CGTCAGCGCCTG\", \"GCTGAGCACCGG\", \"AGTACGGGACAG\"]\n",
    "    print(median_string(dna, k))\n",
    "\n",
    "test_median_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a profile matrix Profile, we can evaluate the probability of every k-mer in a string Text and find a Profile-most probable k-mer in Text, i.e., a k-mer that was most likely to have been generated by Profile among all k-mers in Text. For example, ACGGGGATTACC is the Profile-most probable 12-mer in GGTACGGGGATTACCT. Indeed, every other 12-mer in this string has probability 0. In general, if there are multiple Profile-most probable k-mers in Text, then we select the first such k-mer occurring in Text. We will use the Profile-most probable k-mer to greedily guide our next step in our motif-finding algorithm.\n",
    "\n",
    "# Profile-most Probable k-mer Problem: Find a Profile-most probable k-mer in a string.\n",
    "\n",
    "# Input: A string Text, an integer k, and a 4 × k matrix Profile.\n",
    "# Output: A Profile-most probable k-mer in Text.\n",
    "# Code Challenge: solve the Profile-most Probable k-mer Problem.\n",
    "\n",
    "# Sample Input:\n",
    "\n",
    "# ACCTGTTTATTGCCTAAGTTCCGAACAAACCCAATATAGCCCGAGGGCCT\n",
    "# 5\n",
    "# 0.2 0.2 0.3 0.2 0.3\n",
    "# 0.4 0.3 0.1 0.5 0.1\n",
    "# 0.3 0.3 0.5 0.2 0.4\n",
    "# 0.1 0.2 0.1 0.1 0.2\n",
    "# Sample Output:\n",
    "\n",
    "# CCGAG\n",
    "\n",
    "import sys\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your profile_most_probable_kmer function here, along with any subroutines you need\n",
    "\n",
    "def profile_most_probable_kmer(text: str, k: int,\n",
    "                               profile: list[dict[str, float]]) -> str:\n",
    "    \"\"\"\n",
    "    Identifies the most probable k-mer according to a given profile matrix.\n",
    "    \n",
    "    Args:\n",
    "    text (str): The DNA string\n",
    "    k (int): The length of k-mer\n",
    "    profile (list[dict[str, float]]): The profile matrix as a list of dictionaries\n",
    "    \n",
    "    Returns:\n",
    "    str: The most probable k-mer\n",
    "    \"\"\"\n",
    "    max_prob = -1\n",
    "    most_probable = text[:k]\n",
    "    for i in range(len(text) - k + 1):\n",
    "        pattern = text[i:i+k]\n",
    "        prob = 1\n",
    "        for j, base in enumerate(pattern):\n",
    "            prob *= profile[j][base]\n",
    "        if prob > max_prob:\n",
    "            max_prob = prob\n",
    "            most_probable = pattern\n",
    "    return most_probable\n",
    "\n",
    "# Test with the sample input\n",
    "def test_profile_most_probable_kmer():\n",
    "    text = \"ACCTGTTTATTGCCTAAGTTCCGAACAAACCCAATATAGCCCGAGGGCCT\"\n",
    "    k = 5\n",
    "    profile = [\n",
    "        {\"A\": 0.2, \"C\": 0.2, \"G\": 0.3, \"T\": 0.2, \"U\": 0.3},\n",
    "        {\"A\": 0.4, \"C\": 0.3, \"G\": 0.1, \"T\": 0.5, \"U\": 0.1},\n",
    "        {\"A\": 0.3, \"C\": 0.3, \"G\": 0.5, \"T\": 0.2, \"U\": 0.4},\n",
    "        {\"A\": 0.1, \"C\": 0.2, \"G\": 0.1, \"T\": 0.1, \"U\": 0.2}\n",
    "    ]\n",
    "    print(profile_most_probable_kmer(text, k, profile))\n",
    "\n",
    "test_profile_most_probable_kmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List, Dict, Iterable\n",
    "\n",
    "def genome_path(path: List[str]) -> str:\n",
    "    \"\"\"Forms the genome path formed by a collection of patterns.\"\"\"\n",
    "    if not path:\n",
    "        return \"\"\n",
    "    \n",
    "    result = path[0]\n",
    "    k = len(path[0])\n",
    "    \n",
    "    for i in range(1, len(path)):\n",
    "        result += path[i][-1]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# AAT  ATG  ATG  ATG  CAT  CCA  GAT  GCC  GGA  GGG  GTT  TAA  TGC  TGG  TGT\n",
    "\n",
    "# Test the function\n",
    "def test_genome_path():\n",
    "    path = [\"TAA\", \"ATG\", \"ATG\", \"ATG\", \"CAT\", \"CCA\", \"GAT\", \"GCC\", \"GGA\", \"GGG\", \"GTT\", \"AAT\", \"TGC\", \"TGG\", \"TGT\"]\n",
    "    print(genome_path(path))\n",
    "\n",
    "test_genome_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_kd_mer_composition(sequence, k=3, d=2):\n",
    "    # Generate all k-d-mers using list comprehension\n",
    "    # desrbiving the line below: \n",
    "    # 1. iterate over all possible starting positions of the first k-mer\n",
    "    # 2. for each starting position, extract the first k-mer and the second k-mer\n",
    "    # 3. join the two k-mers with a pipe character\n",
    "    # 4. enclose the joined k-mers in parentheses\n",
    "    # 5. repeat for all possible starting positions\n",
    "    kd_mers = [f\"({sequence[i:i+k]}|{sequence[i+k+d:i+2*k+d]})\" for i in range(len(sequence)-2*k-d+1)]\n",
    "    \n",
    "    # Sort the k-d-mers lexicographically\n",
    "    kd_mers.sort()\n",
    "    \n",
    "    # Join the k-d-mers into a single string\n",
    "    result = \" \".join(kd_mers)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "sequence = \"TAATGCCATGGGATGTT\"\n",
    "result = generate_kd_mer_composition(sequence)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import List, Dict, Iterable\n",
    "\n",
    "# Please do not remove package declarations because these are used by the autograder.\n",
    "\n",
    "# Insert your eulerian_cycle function here, along with any subroutines you need\n",
    "# g[u] is the list of neighbors of the vertex u\n",
    "def eulerian_cycle(g: Dict[int, List[int]]) -> Iterable[int]:\n",
    "    \"\"\"Constructs an Eulerian cycle in a graph.\"\"\"\n",
    "    def visit(v):\n",
    "        while g[v]:\n",
    "            visit(g[v].pop())\n",
    "        cycle.append(v)\n",
    "    \n",
    "    cycle = []\n",
    "    visit(next(iter(g)))\n",
    "    return cycle[::-1]\n",
    "\n",
    "# Sample Input:\n",
    "\n",
    "# 0: 3\n",
    "# 1: 0\n",
    "# 2: 1 6\n",
    "# 3: 2\n",
    "# 4: 2\n",
    "# 5: 4\n",
    "# 6: 5 8\n",
    "# 7: 9\n",
    "# 8: 7\n",
    "# 9: 6\n",
    "# Sample Output:\n",
    "\n",
    "# 0 3 2 6 8 7 9 6 5 4 2 1 0\n",
    "\n",
    "# Test the function\n",
    "def test_eulerian_cycle():\n",
    "    g = {\n",
    "        0: [3],\n",
    "        1: [0],\n",
    "        2: [1, 6],\n",
    "        3: [2],\n",
    "        4: [2],\n",
    "        5: [4],\n",
    "        6: [5, 8],\n",
    "        7: [9],\n",
    "        8: [7],\n",
    "        9: [6]\n",
    "    }\n",
    "    print(\" \".join(map(str, eulerian_cycle(g))))\n",
    "\n",
    "test_eulerian_cycle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "# Constructs a genome from a path of k-mers\n",
    "def genome_path(path: List[str]) -> str:\n",
    "    \"\"\"Reconstructs a genome from a given path of k-mers.\"\"\"\n",
    "    genome = path[0]\n",
    "    for pattern in path[1:]:\n",
    "        genome += pattern[-1]\n",
    "    return genome\n",
    "\n",
    "# Constructs a De Bruijn graph from a list of k-mers\n",
    "def de_bruijn_kmers(k_mers: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"Constructs a De Bruijn graph from a list of k-mers.\"\"\"\n",
    "    adj_list = defaultdict(list)\n",
    "    for kmer in k_mers:\n",
    "        node1, node2 = kmer[:-1], kmer[1:]\n",
    "        adj_list[node1].append(node2)\n",
    "    return dict(adj_list)\n",
    "\n",
    "# Extends the Eulerian cycle in the graph\n",
    "def extend_cycle(cycle: List[str], marked_graph: Dict[str, List[str]]) -> List[str]:\n",
    "    \"\"\"Extends the Eulerian cycle from a given node in the marked graph.\"\"\"\n",
    "    if cycle:\n",
    "        cycle.pop()  # remove the repeated node at the end\n",
    "        new_start_index = next(i for i, node in enumerate(cycle) if node in marked_graph)\n",
    "        cycle = cycle[new_start_index:] + cycle[:new_start_index]\n",
    "        cycle.append(cycle[0])  # re-add the repeated node\n",
    "        current_node = cycle[-1]\n",
    "    else:\n",
    "        current_node = next(iter(marked_graph))  # get an arbitrary node from the graph\n",
    "        cycle = [current_node]\n",
    "    \n",
    "    while current_node in marked_graph:\n",
    "        old_node = current_node\n",
    "        current_node = marked_graph[old_node].pop()\n",
    "        if not marked_graph[old_node]:\n",
    "            del marked_graph[old_node]  # remove the node if no more edges\n",
    "        cycle.append(current_node)\n",
    "    \n",
    "    return cycle\n",
    "\n",
    "# Constructs an Eulerian cycle in a graph\n",
    "def eulerian_cycle(g: Dict[str, List[str]]) -> List[str]:\n",
    "    \"\"\"Constructs an Eulerian cycle in a graph. Assumes the graph is Eulerian and connected.\"\"\"\n",
    "    cycle = []\n",
    "    while g:\n",
    "        cycle = extend_cycle(cycle, g)\n",
    "    return cycle\n",
    "\n",
    "# Fixes unbalanced nodes in a graph to make it Eulerian\n",
    "def fix_unbalanced(g: Dict[str, List[str]]) -> tuple[str, str]:\n",
    "    \"\"\"Finds and fixes unbalanced nodes in the graph.\"\"\"\n",
    "    total_degree = defaultdict(int)\n",
    "    \n",
    "    for node1, adj_nodes in g.items():\n",
    "        for node2 in adj_nodes:\n",
    "            total_degree[node1] += 1  # Out-degree\n",
    "            total_degree[node2] -= 1  # In-degree\n",
    "\n",
    "    s, t = None, None\n",
    "    for node, tot_degree in total_degree.items():\n",
    "        if tot_degree == 1:\n",
    "            t = node\n",
    "        elif tot_degree== -1:\n",
    "            s = node\n",
    "\n",
    "    if s and t:\n",
    "        g.setdefault(s, []).append(t)\n",
    "    \n",
    "    return s, t\n",
    "\n",
    "# Constructs an Eulerian path in a graph\n",
    "def eulerian_path(g: Dict[str, List[str]]) -> List[str]:\n",
    "    \"\"\"Constructs an Eulerian path in a graph, assuming the graph is nearly Eulerian.\"\"\"\n",
    "    s, t = fix_unbalanced(g)\n",
    "    cycle = eulerian_cycle(g)\n",
    "    \n",
    "    if s:\n",
    "        cycle.pop()  # Remove the duplicate last node\n",
    "        t_index = next(i for i, (u, v) in enumerate(zip(cycle, cycle[1:])) if u == s and v == t)\n",
    "        cycle = cycle[t_index + 1:] + cycle[:t_index + 1]\n",
    "    \n",
    "    return cycle\n",
    "\n",
    "# Reconstructs a string from its k-mer composition\n",
    "def string_reconstruction(patterns: List[str], k: int) -> str:\n",
    "    \"\"\"Reconstructs a string from its k-mer composition.\"\"\"\n",
    "    # Step 1: Construct the de Bruijn graph from the patterns\n",
    "    de_bruijn_graph = de_bruijn_kmers(patterns)\n",
    "    \n",
    "    # Step 2: Find the Eulerian path in the de Bruijn graph\n",
    "    path = eulerian_path(de_bruijn_graph)\n",
    "    \n",
    "    # Step 3: Reconstruct the string from the path\n",
    "    return genome_path(path)\n",
    "\n",
    "\n",
    "# Test the function\n",
    "# Sample Input:\n",
    "\n",
    "# 3\n",
    "# ACG CGT GTG TGT GTA TAT ATA\n",
    "# Sample Output:\n",
    "\n",
    "# ACGTGTATA\n",
    "\n",
    "# def test_string_reconstruction():\n",
    "#     k = 3\n",
    "#     patterns = [\"ACG\", \"CGT\", \"GTG\", \"TGT\", \"GTA\", \"TAT\", \"ATA\"]\n",
    "#     print(string_reconstruction(patterns, k))\n",
    "\n",
    "# test_string_reconstruction()\n",
    "\n",
    "kmer_file = \"data/kmers_25.txt\"\n",
    "k = 25\n",
    "with open(kmer_file, 'r') as f:\n",
    "    patterns = f.read().splitlines()\n",
    "\n",
    "# write ot the output file\n",
    "output_file = \"data/kmers25_output.txt\"\n",
    "reconstructed_string = string_reconstruction(patterns, k)\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(reconstructed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "102100\n"
     ]
    }
   ],
   "source": [
    "# check diff between these 2 files\n",
    "kmer_file100 = \"data/kmers100_output.txt\"\n",
    "kmer_file25 = \"data/kmers25_output.txt\"\n",
    "# they are single lines\n",
    "with open(kmer_file100, 'r') as f:\n",
    "    kmer100 = f.read()\n",
    "with open(kmer_file25, 'r') as f:\n",
    "    kmer25 = f.read()\n",
    "print(kmer100 == kmer25)\n",
    "# get difference between the two strings\n",
    "diff = [i for i in range(len(kmer100)) if kmer100[i] != kmer25[i]]\n",
    "print(len(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reconstructed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Iterable\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "\n",
    "def de_bruijn_kmers(k_mers: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"Constructs a De Bruijn graph from a list of k-mers.\"\"\"\n",
    "    adj_list = defaultdict(list)\n",
    "    for kmer in k_mers:\n",
    "        node1, node2 = kmer[:-1], kmer[1:]\n",
    "        adj_list[node1].append(node2)\n",
    "    return dict(adj_list)\n",
    "\n",
    "def calculate_degrees(graph: Dict[str, List[str]]) -> Dict[str, Dict[str, int]]:\n",
    "    \"\"\"Calculates the in-degree and out-degree of each node.\"\"\"\n",
    "    degrees = defaultdict(lambda: {'in': 0, 'out': 0})\n",
    "    for node1, adj_nodes in graph.items():\n",
    "        degrees[node1]['out'] += len(adj_nodes)\n",
    "        for node2 in adj_nodes:\n",
    "            degrees[node2]['in'] += 1\n",
    "    return degrees\n",
    "\n",
    "def contig_generation(Patterns: List[str]) -> List[str]:\n",
    "    \"\"\"Generates contigs from a set of k-mers.\"\"\"\n",
    "    graph = de_bruijn_kmers(Patterns)\n",
    "    \n",
    "    degrees = calculate_degrees(graph)\n",
    "    \n",
    "    contigs = []\n",
    "    \n",
    "    for node in graph:\n",
    "        # If the node is a \"branching\" node (in-degree != 1 or out-degree != 1) or isolated\n",
    "        if degrees[node]['in'] != 1 or degrees[node]['out'] != 1:\n",
    "            if degrees[node]['out'] > 0:\n",
    "                for neighbor in graph[node]:\n",
    "                    contig = [node, neighbor]\n",
    "                    # Follow the path until a branching or isolated node is reached\n",
    "                    while degrees[neighbor]['in'] == 1 and degrees[neighbor]['out'] == 1:\n",
    "                        next_node = graph[neighbor][0]\n",
    "                        contig.append(next_node)\n",
    "                        neighbor = next_node\n",
    "                    contigs.append(''.join([contig[0]] + [c[-1] for c in contig[1:]]))\n",
    "    \n",
    "    return contigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 173802, 173802)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run conig_generation on the kmers_25.txt file\n",
    "\n",
    "kmer_file = \"data/kmers_100.txt\"\n",
    "with open(kmer_file, 'r') as f:\n",
    "    patterns = f.read().splitlines()\n",
    "\n",
    "# write ot the output file\n",
    "output_file = \"data/kmers100_contigs.txt\"\n",
    "contigs = contig_generation(patterns)\n",
    "with open(output_file, 'w') as f:\n",
    "    f.write(\"\\n\".join(contigs))\n",
    "\n",
    "# get max length str from contigs\n",
    "max_contig = max(contigs, key=len)\n",
    "# get min length str from contigs\n",
    "min_contig = min(contigs, key=len)\n",
    "len(contigs), len(max_contig), len(min_contig)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: ('TAATATATATTATAATATTATTAT', 'CTAAACAAATAATTTTAAATTCAA')\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from typing import List, Dict, Iterable, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "def read_paired_reads_from_file(filename: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Reads paired reads from a file and returns them as a list of tuples.\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        return [tuple(line.strip().split('|')) for line in f]\n",
    "\n",
    "def StringReconstructionReadPairs(PairedReads: List[Tuple[str, str]],\n",
    "                                  k: int, d: int) -> str:\n",
    "    \"\"\"\n",
    "    Reconstructs a string from its (k,d)-mer composition.\n",
    "    \n",
    "    Args:\n",
    "    PairedReads: List of tuples, each containing a pair of k-mers\n",
    "    k: Length of each k-mer in the paired reads\n",
    "    d: Gap between paired k-mers\n",
    "    \n",
    "    Returns:\n",
    "    Reconstructed string\n",
    "    \"\"\"\n",
    "    # Construct the paired de Bruijn graph\n",
    "    graph = construct_paired_de_bruijn_graph(PairedReads, k)\n",
    "    \n",
    "    # Find Eulerian path in the graph\n",
    "    path = find_eulerian_path(graph)\n",
    "    \n",
    "    # Reconstruct the string from the path\n",
    "    return reconstruct_from_paired_path(path, k, d)\n",
    "\n",
    "def construct_paired_de_bruijn_graph(PairedReads: List[Tuple[str, str]], k: int) -> Dict[Tuple[str, str], List[Tuple[str, str]]]:\n",
    "    \"\"\"Constructs a paired de Bruijn graph from the read-pairs.\"\"\"\n",
    "    graph = defaultdict(list)\n",
    "    for read1, read2 in PairedReads:\n",
    "        for i in range(len(read1) - k + 1):\n",
    "            prefix1, suffix1 = read1[i:i+k-1], read1[i+1:i+k]\n",
    "            prefix2, suffix2 = read2[i:i+k-1], read2[i+1:i+k]\n",
    "            graph[(prefix1, prefix2)].append((suffix1, suffix2))\n",
    "    return dict(graph)\n",
    "\n",
    "def find_eulerian_path(graph: Dict[Tuple[str, str], List[Tuple[str, str]]]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Finds an Eulerian path in the paired de Bruijn graph.\"\"\"\n",
    "    in_degree = defaultdict(int)\n",
    "    for node, edges in graph.items():\n",
    "        for edge in edges:\n",
    "            in_degree[edge] += 1\n",
    "    \n",
    "    start = next((node for node in graph if len(graph[node]) > in_degree[node]), None)\n",
    "    end = next((node for node in in_degree if in_degree[node] > len(graph.get(node, []))), None)\n",
    "\n",
    "    if start is None or end is None:\n",
    "        raise ValueError(\"Unable to find start or end node for Eulerian path\")\n",
    "\n",
    "    if end not in graph:\n",
    "        graph[end] = []\n",
    "    graph[end].append(start)\n",
    "\n",
    "    cycle = find_eulerian_cycle(graph)\n",
    "\n",
    "    # Rotate cycle to start with the correct edge\n",
    "    start_index = next((i for i, (u, v) in enumerate(zip(cycle, cycle[1:])) if u == end and v == start), None)\n",
    "    if start_index is None:\n",
    "        raise ValueError(\"Unable to find the correct starting point in the Eulerian cycle\")\n",
    "    \n",
    "    return cycle[start_index + 1:] + cycle[1:start_index + 1]\n",
    "\n",
    "def find_eulerian_cycle(graph: Dict[Tuple[str, str], List[Tuple[str, str]]]) -> List[Tuple[str, str]]:\n",
    "    \"\"\"Finds an Eulerian cycle in the given graph.\"\"\"\n",
    "    stack = []\n",
    "    cycle = []\n",
    "    current = next(iter(graph))\n",
    "    while stack or graph[current]:\n",
    "        if not graph[current]:\n",
    "            cycle.append(current)\n",
    "            current = stack.pop()\n",
    "        else:\n",
    "            stack.append(current)\n",
    "            next_node = graph[current].pop()\n",
    "            current = next_node\n",
    "    return cycle[::-1]\n",
    "\n",
    "def reconstruct_from_paired_path(path: List[Tuple[str, str]], k: int, d: int) -> str:\n",
    "    \"\"\"Reconstructs the string from the Eulerian path of paired k-mers.\"\"\"\n",
    "    left_path = [pair[0] for pair in path]\n",
    "    right_path = [pair[1] for pair in path]\n",
    "    \n",
    "    genome = left_path[0]\n",
    "    for node in left_path[1:]:\n",
    "        genome += node[-1]\n",
    "    \n",
    "    for i in range(k + d):\n",
    "        genome += right_path[i][-1]\n",
    "    \n",
    "    return genome\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = \"data/paired_end_reads.txt\"\n",
    "    PairedReads = read_paired_reads_from_file(input_file)\n",
    "    k = 25  # Adjusted k-mer length for de Bruijn graph construction\n",
    "    d = 1000  # gap between paired k-mers\n",
    "    \n",
    "    try:\n",
    "        result = StringReconstructionReadPairs(PairedReads, k, d)\n",
    "        print(f\"Length of reconstructed genome: {len(result)}\")\n",
    "        print(f\"First 100 nucleotides: {result[:100]}\")\n",
    "        print(f\"Last 100 nucleotides: {result[-100:]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for skew plot\n",
    "\n",
    "def skew_plot(genome: str) -> list[int]:\n",
    "    \"\"\"Calculate the skew of a genome.\"\"\"\n",
    "    skew = [0]\n",
    "    for i in range(len(genome)):\n",
    "        if genome[i] == \"C\":\n",
    "            skew.append(skew[i] - 1)\n",
    "        elif genome[i] == \"G\":\n",
    "            skew.append(skew[i] + 1)\n",
    "        else:\n",
    "            skew.append(skew[i])\n",
    "    return skew\n",
    "\n",
    "# plot the skew of the genome\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "file_loc = \"data/Salmonella_enterica.txt\"\n",
    "with open(file_loc, 'r') as f:\n",
    "    genome = f.read()\n",
    "\n",
    "def plot_skew():\n",
    "    # read genome file\n",
    "    skew = skew_plot(genome)\n",
    "    plt.plot(skew)\n",
    "    plt.show()\n",
    "\n",
    "plot_skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the skew array of the genome\n",
    "skew = skew_plot(genome)\n",
    "# get the index of the minimum skew value\n",
    "min_index = skew.index(min(skew))\n",
    "# get the index of the maximum skew value\n",
    "max_index = skew.index(max(skew))\n",
    "# print the indices of the minimum and maximum skew values\n",
    "print(min_index, max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run frequent_words on the genome between the minimum and maximum skew values\n",
    "# this will give us the most frequent 9-mers in the genome\n",
    "frequent_words(genome[min_index:max_index], 9)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mathenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
